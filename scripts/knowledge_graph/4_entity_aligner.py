import json
import os
import sys
from collections import defaultdict

import faiss
import pandas as pd
from sentence_transformers import SentenceTransformer

from silver_pilot.config import config
from silver_pilot.utils.log import LogManager

# ================= é…ç½®åŒºåŸŸ =================
# 1. è·¯å¾„é…ç½®
CMEKG_CSV_PATH = config.DATA_DIR / "raw/datasets/full_export.csv"  # ä½ çš„CMeKGå…¨é‡èŠ‚ç‚¹æ–‡ä»¶
EXTRACTED_JSON_PATH = config.DATA_DIR / "processed/KG/triplets"
OUTPUT_CSV_PATH = config.DATA_DIR / "processed/KG/entity_alignment/entity_alignment_results.csv"
LOG_FILE_DIR = config.DATA_DIR / "processed/KG/entity_alignment"

# 2. æ¨¡å‹é…ç½®
MODEL_NAME = "paraphrase-multilingual-MiniLM-L12-v2"  # æˆ– 'shibing624/text2vec-base-chinese'
BATCH_SIZE = 64  # è®¡ç®—å‘é‡æ—¶çš„æ‰¹æ¬¡å¤§å°ï¼Œæ˜¾å­˜/å†…å­˜ä¸å¤Ÿå¯è°ƒå°

# 3. åŒ¹é…é€»è¾‘
SIMILARITY_THRESHOLD = 0.85  # ç›¸ä¼¼åº¦é˜ˆå€¼

# 4. æ—¥å¿—é…ç½®
log_manager = LogManager(LOG_FILE_DIR, "EntityAligner")
logger = log_manager.get_logger()
# ===========================================


class EntityAligner:
    def __init__(self, model_name: str, threshold: float):
        self.threshold = threshold
        logger.info(f"æ­£åœ¨åŠ è½½è¯­ä¹‰æ¨¡å‹: {model_name} ...")
        # device='cpu' æˆ– 'cuda'ï¼Œåº“ä¼šè‡ªåŠ¨é€‰æ‹©
        self.model = SentenceTransformer(model_name)
        logger.info("æ¨¡å‹åŠ è½½å®Œæˆã€‚")

    def normalize_label(self, label: str) -> str:
        """æ¸…æ´—Labelæ ¼å¼ï¼Œå»é™¤å†’å·ï¼Œå¹¶å¤„ç†éæ³•è¾“å…¥"""
        # 1. å¤„ç†ç©ºå€¼ (NaN, None)
        if pd.isna(label) or label == "":
            return "Unknown"

        # 2. å¼ºåˆ¶è½¬ä¸ºå­—ç¬¦ä¸² (é˜²æ­¢ CSV é‡ŒæŸäº› label è¢«è¯¯è¯»ä¸ºæ•°å­—)
        label_str = str(label)

        # 3. å»é™¤å†’å·å’Œé¦–å°¾ç©ºæ ¼
        return label_str.replace(":", "").strip()

    def load_cmekg_data(self, csv_path: str) -> dict[str, pd.DataFrame]:
        """
        åˆ†å—åŠ è½½å¤§å‹CSVï¼Œå¹¶æŒ‰Labelåˆ†ç»„å­˜å‚¨ (å·²å¢åŠ ç©ºå€¼å®¹é”™å¤„ç†)
        """
        logger.info(f"å¼€å§‹åŠ è½½ CMeKG æ•°æ®: {csv_path}")
        if not os.path.exists(csv_path):
            logger.error(f"æ–‡ä»¶æœªæ‰¾åˆ°: {csv_path}")
            sys.exit(1)

        grouped_data = defaultdict(list)
        try:
            chunk_count = 0
            # encoding='utf-8-sig' å¯ä»¥è‡ªåŠ¨å¤„ç† BOM å¤´ï¼Œæ¨èåŠ ä¸Š
            for chunk in pd.read_csv(
                csv_path, chunksize=50000, encoding="utf-8-sig", on_bad_lines="skip"
            ):
                # 1. åŠ¨æ€å¯»æ‰¾åˆ—å (å®¹é”™å¤„ç†)
                cols = chunk.columns
                name_col = next((c for c in cols if "name" in c.lower()), "name")
                label_col = next((c for c in cols if "label" in c.lower()), "label")
                id_col = next((c for c in cols if "id" in c.lower()), "id")

                # ================= å…³é”®ä¿®å¤å¼€å§‹ =================
                # 2. é¢„å¤„ç†ï¼šå¡«å……ç©ºå€¼å¹¶å¼ºè½¬ä¸ºå­—ç¬¦ä¸²ï¼Œé¿å… AttributeError
                chunk[label_col] = chunk[label_col].fillna("Unknown").astype(str)
                # ================= å…³é”®ä¿®å¤ç»“æŸ =================

                # 3. åº”ç”¨æ¸…æ´—å‡½æ•°
                chunk["clean_label"] = chunk[label_col].apply(self.normalize_label)

                # 4. ç­›é€‰æœ‰æ•ˆæ•°æ® (è¿‡æ»¤æ‰ Unknown çš„è¡Œï¼Œé¿å…æ— æ•ˆè®¡ç®—)
                # ä»…ä¿ç•™éœ€è¦çš„åˆ—
                subset = chunk[[id_col, name_col, "clean_label"]].rename(
                    columns={id_col: "id", name_col: "name", "clean_label": "label"}
                )

                # è¿‡æ»¤æ‰æ ‡ç­¾è§£æå¤±è´¥çš„æ•°æ®
                subset = subset[subset["label"] != "Unknown"]

                # 5. åˆ†ç»„å­˜æ”¾
                for label, group in subset.groupby("label"):
                    grouped_data[label].append(group)

                chunk_count += 1
                if chunk_count % 10 == 0:
                    logger.info(f"å·²å¤„ç† {chunk_count * 5}ä¸‡ è¡Œæ•°æ®...")

            # åˆå¹¶æ¯ä¸ªLabelä¸‹çš„DataFrame
            final_groups = {}
            total_nodes = 0
            for label, df_list in grouped_data.items():
                if df_list:  # ç¡®ä¿éç©º
                    final_groups[label] = pd.concat(df_list, ignore_index=True)
                    total_nodes += len(final_groups[label])

            logger.info(
                f"CMeKG æ•°æ®åŠ è½½å®Œæ¯•ã€‚å…± {total_nodes} ä¸ªæœ‰æ•ˆèŠ‚ç‚¹ï¼Œæ¶‰åŠ {len(final_groups)} ä¸ªæ ‡ç­¾ã€‚"
            )
            return final_groups

        except Exception as e:
            # æ‰“å°å®Œæ•´çš„é”™è¯¯å †æ ˆï¼Œæ–¹ä¾¿è°ƒè¯•
            import traceback

            logger.error(f"è¯»å– CSV å¤±è´¥: {e}")
            logger.error(traceback.format_exc())
            sys.exit(1)

    def load_extracted_data(self, json_path: str) -> dict[str, list[str]]:
        """
        åŠ è½½JSONæ•°æ®ï¼ˆæ”¯æŒå•æ–‡ä»¶æˆ–æ–‡ä»¶å¤¹ï¼‰ï¼Œæå–å”¯ä¸€å®ä½“å¹¶æŒ‰Labelåˆ†ç»„
        """
        logger.info(f"å¼€å§‹åŠ è½½æå–æ•°æ®: {json_path}")

        # 1. ç¡®å®šè¦å¤„ç†çš„æ–‡ä»¶åˆ—è¡¨
        files_to_process = []

        if os.path.isfile(json_path):
            # æƒ…å†µA: ä¼ å…¥çš„æ˜¯å•ä¸ªæ–‡ä»¶
            files_to_process.append(json_path)
        elif os.path.isdir(json_path):
            # æƒ…å†µB: ä¼ å…¥çš„æ˜¯æ–‡ä»¶å¤¹ -> è¯»å–ä¸‹é¢æ‰€æœ‰æ–‡ä»¶
            for file_name in os.listdir(json_path):
                full_path = os.path.join(json_path, file_name)
                # ç¡®ä¿æ˜¯æ–‡ä»¶è€Œä¸æ˜¯å­æ–‡ä»¶å¤¹
                if os.path.isfile(full_path):
                    files_to_process.append(full_path)
        else:
            logger.error(f"è·¯å¾„ä¸å­˜åœ¨: {json_path}")
            return {}

        extracted_map = defaultdict(set)
        valid_files_count = 0
        total_triples = 0

        # 2. éå†è¯»å–
        for file_path in files_to_process:
            try:
                with open(file_path, encoding="utf-8") as f:
                    # å°è¯•è§£æ JSON
                    data = json.load(f)

                    # å…¼å®¹æ•°æ®å¯èƒ½æ˜¯ list æˆ– dict çš„æƒ…å†µ
                    if isinstance(data, dict):
                        data = [data]  # ç»Ÿä¸€è½¬ä¸º list å¤„ç†

                    # æå–é€»è¾‘
                    for item in data:
                        if "extraction" in item and "triples" in item["extraction"]:
                            for t in item["extraction"]["triples"]:
                                s_label = self.normalize_label(t.get("start_label", "Unknown"))
                                e_label = self.normalize_label(t.get("end_label", "Unknown"))

                                extracted_map[s_label].add(t["start"])
                                extracted_map[e_label].add(t["end"])
                                total_triples += 1

                    valid_files_count += 1

            except json.JSONDecodeError:
                logger.warning(f"âš ï¸ è·³è¿‡éJSONæ–‡ä»¶æˆ–æ ¼å¼é”™è¯¯: {os.path.basename(file_path)}")
            except Exception as e:
                logger.error(f"âŒ è¯»å–æ–‡ä»¶å‡ºé”™ {os.path.basename(file_path)}: {e}")

        # 3. ç»“æœè½¬æ¢
        final_map = {k: list(v) for k, v in extracted_map.items()}
        logger.info(f"æ•°æ®åŠ è½½å®Œæ¯•ã€‚æˆåŠŸè¯»å–æ–‡ä»¶: {valid_files_count}/{len(files_to_process)} ä¸ªã€‚")
        logger.info(f"å…±æå–ä¸‰å…ƒç»„ {total_triples} ä¸ªï¼Œæ¶‰åŠæ ‡ç­¾: {list(final_map.keys())}")

        return final_map

    def align_vectors_faiss(self, extracted_names: list[str], cmekg_df: pd.DataFrame) -> list[dict]:
        """
        æ ¸å¿ƒæ–¹æ³•ï¼šä½¿ç”¨ Faiss åœ¨ç‰¹å®š Label åˆ†ç»„å†…è¿›è¡Œæ£€ç´¢
        """
        # 1. å‡†å¤‡ CMeKG å‘é‡ (Database)
        db_names = cmekg_df["name"].tolist()
        # æ³¨æ„ï¼šè¿™é‡Œå¦‚æœ db_names ä¹Ÿæ˜¯ç™¾ä¸‡çº§ï¼Œmodel.encode å»ºè®®ä¹ŸåŠ  batch_size
        logger.info(f"  æ­£åœ¨è®¡ç®— {len(db_names)} ä¸ªæ•°æ®åº“å®ä½“çš„å‘é‡...")
        db_embeddings = self.model.encode(
            db_names, batch_size=BATCH_SIZE, show_progress_bar=False, convert_to_numpy=True
        )

        # 2. å‡†å¤‡ æå–å®ä½“ å‘é‡ (Query)
        query_embeddings = self.model.encode(
            extracted_names, batch_size=BATCH_SIZE, show_progress_bar=False, convert_to_numpy=True
        )

        # 3. å½’ä¸€åŒ– (L2 Normalization) -> ä½¿å¾—å†…ç§¯ç­‰äºä½™å¼¦ç›¸ä¼¼åº¦
        faiss.normalize_L2(db_embeddings)
        faiss.normalize_L2(query_embeddings)

        # 4. æ„å»º Faiss ç´¢å¼•
        dim = db_embeddings.shape[1]
        index = faiss.IndexFlatIP(dim)  # Inner Product
        index.add(db_embeddings)

        # 5. æ£€ç´¢ (Top 1)
        # D: Distances (Similarity Scores), I: Indices
        D, I = index.search(query_embeddings, 1)

        results = []
        for i, name in enumerate(extracted_names):
            score = float(D[i][0])
            match_idx = int(I[i][0])

            match_record = cmekg_df.iloc[match_idx]
            match_name = match_record["name"]
            match_id = match_record["id"]

            action = "ğŸ†• Create New"
            if score >= self.threshold:
                action = "ğŸ”— Link Existing"

            results.append(
                {
                    "extracted_name": name,
                    "matched_name": match_name,
                    "matched_id": match_id,
                    "score": round(score, 4),
                    "action": action,
                }
            )

        return results

    def run(self) -> None:
        # 1. åŠ è½½æ•°æ®
        cmekg_groups = self.load_cmekg_data(CMEKG_CSV_PATH)
        extracted_groups = self.load_extracted_data(EXTRACTED_JSON_PATH)

        all_results = []

        # 2. éå†æå–å‡ºæ¥çš„æ¯ä¸€ä¸ª Label ç»„
        for label, extracted_names in extracted_groups.items():
            logger.info(f"æ­£åœ¨å¤„ç†æ ‡ç­¾ç»„: [{label}] (å…± {len(extracted_names)} ä¸ªå®ä½“)...")

            if label in cmekg_groups:
                # å‘½ä¸­ï¼šè¯¥æ ‡ç­¾åœ¨æ•°æ®åº“ä¸­å­˜åœ¨ï¼Œè¿›è¡Œ Faiss æ£€ç´¢
                cmekg_df = cmekg_groups[label]
                logger.info(f"  -> æ•°æ®åº“ä¸­ [{label}] å…±æœ‰ {len(cmekg_df)} ä¸ªå€™é€‰å®ä½“ã€‚")

                group_results = self.align_vectors_faiss(extracted_names, cmekg_df)

                # è¿½åŠ  Label ä¿¡æ¯
                for res in group_results:
                    res["label"] = label
                all_results.extend(group_results)

            else:
                # æœªå‘½ä¸­ï¼šæ•°æ®åº“é‡Œæ ¹æœ¬æ²¡è¿™ä¸ªæ ‡ç­¾ï¼Œç›´æ¥å…¨åˆ¤å®šä¸ºæ–°å®ä½“
                logger.warning(f"  -> æ•°æ®åº“ä¸­ä¸å­˜åœ¨æ ‡ç­¾ [{label}]ï¼Œè¯¥ç»„æ‰€æœ‰å®ä½“æ ‡è®°ä¸ºæ–°å®ä½“ã€‚")
                for name in extracted_names:
                    all_results.append(
                        {
                            "extracted_name": name,
                            "matched_name": None,
                            "matched_id": None,
                            "score": 0.0,
                            "action": "ğŸ†• Create New",
                            "label": label,
                        }
                    )

        # 3. å¯¼å‡ºç»“æœ
        if all_results:
            df_out = pd.DataFrame(all_results)
            # è°ƒæ•´åˆ—é¡ºåº
            cols = ["label", "extracted_name", "action", "score", "matched_name", "matched_id"]
            df_out = df_out[cols].sort_values(by=["label", "score"], ascending=[True, False])

            df_out.to_csv(OUTPUT_CSV_PATH, index=False, encoding="utf-8-sig")
            logger.info(f"ğŸ‰ å¤„ç†å®Œæˆï¼ç»“æœå·²ä¿å­˜è‡³: {OUTPUT_CSV_PATH}")

            # ç®€å•çš„ç»Ÿè®¡æ—¥å¿—
            link_count = len(df_out[df_out["action"].str.contains("Link")])
            new_count = len(df_out) - link_count
            logger.info(f"ç»Ÿè®¡: ğŸ”— é“¾æ¥å·²æœ‰èŠ‚ç‚¹: {link_count} | ğŸ†• æ–°å»ºèŠ‚ç‚¹: {new_count}")
        else:
            logger.warning("æ²¡æœ‰äº§ç”Ÿä»»ä½•ç»“æœã€‚")


if __name__ == "__main__":
    # å®ä¾‹åŒ–å¹¶è¿è¡Œ
    # è¯·ç¡®ä¿æ­¤æ—¶ç›®å½•ä¸‹æœ‰ cmekg_nodes.csv å’Œ ä½ çš„jsonæ–‡ä»¶
    aligner = EntityAligner(model_name=MODEL_NAME, threshold=SIMILARITY_THRESHOLD)
    aligner.run()
